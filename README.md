# The Clean Transformer

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/181hTrhfbmyub7UaMJmBY_fbFfLBCBi58?usp=sharing)

ğŸ‡°ğŸ‡· `pytorch-lightning`ê³¼ `wandb`ë¡œ ê¹”ë”í•˜ê²Œ êµ¬í˜„í•´ë³´ëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ 

ğŸ‡¬ğŸ‡§ Transformer implemented with clean and structured code - much thanks to `pytorch-lightning` & `wandb `!


## Quick Start
ìš°ì„ , ë¦¬í¬ë¥¼ í´ë¡ í•˜ê³  ê°€ìƒí™˜ê²½ì„ êµ¬ì¶•í•©ë‹ˆë‹¤:
```shell
git clone https://github.com/eubinecto/enkorde.git
conda create -n enkorde python=3.9 
conda activate enkorde
conda install pip
cd enkorde
pip install -r requirements.txt
```

ì´í›„ ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ê³ , ê°„ë‹¨í•œ í•œêµ­ì–´ ë²ˆì—­ì„ ì‹œë„í•´ë³´ê¸° ìœ„í•´ `main_infer.py` ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤. 
ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œ í•˜ê¸° ìœ„í•´ì„  ë°˜ë“œì‹œ ì²«ë²ˆì§¸ ì¸ì (`entity`)ë¡œ `eubinecto`ë¥¼ ë„£ì–´ì•¼ í•©ë‹ˆë‹¤.
ì¶”ê°€ë¡œ ì˜ì–´ë¡œ ë²ˆì—­í•˜ê³ ì í•˜ëŠ” í•œêµ­ì–´ ë¬¸ì¥ì„ `--kor` ì¸ìë¡œ ë„£ì–´ì¤ë‹ˆë‹¤. 
```shell
python3 main_infer.py eubinecto --kor="ì¹´í˜ì¸ì€ ì›ë˜ ì»¤í”¼ì— ë“¤ì–´ìˆëŠ” ë¬¼ì§ˆì´ë‹¤."
```

ìœ„ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ë©´, ë‹¤ìŒê³¼ ê°™ì€ ì„ íƒì°½ì´ ëœ¹ë‹ˆë‹¤:
```text
wandb: (1) Create a W&B account
wandb: (2) Use an existing W&B account
wandb: (3) Don't visualize my results
wandb: Enter your choice: 
```

3ì„ ì…ë ¥ í›„ ì—”í„°ë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”. ì´í›„ ì‚¬ì „í•™ìŠµëœ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì´ `enkorde/artifacts/transformer:overfit_small` ì— ë‹¤ìš´ë¡œë“œë˜ë©°, ë‹¤ìŒê³¼ ê°™ì´ ì£¼ì–´ì§„
`--kor` ë¬¸ì¥ì„ ì˜ì–´ë¡œ ë²ˆì—­í•©ë‹ˆë‹¤:
```text
wandb: You chose 'Don't visualize my results'
wandb: Downloading large artifact transformer:overfit_small, 263.49MB. 1 files... Done. 0:0:0
ì¹´í˜ ##ì¸ì€ ì›ë˜ ì»¤í”¼ ##ì— ë“¤ì–´ ##ìˆëŠ” ë¬¼ì§ˆ ##ì´ë‹¤ . -> caf ##fe ##ine is a subst ##ance natural ##ly found in coffee .
```

## í”„ë¡œì íŠ¸ êµ¬ì¡° 
```
.                        # ROOT_DIR 
â”œâ”€â”€ main_build.py        # ì£¼ì–´ì§„ ë§ë­‰ì¹˜ì— ì í•©í•œ huggingface í† í¬ë‚˜ì´ì €ë¥¼ í›ˆë ¨ì‹œí‚¬ ë•Œ ì‚¬ìš©í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ main_train.py        # êµ¬í˜„ëœ íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ í›ˆë ¨ì‹œí‚¬ ë•Œ ì‚¬ìš©í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ main_infer.py        # ì‚¬ì „í•™ìŠµëœ íŠ¸ëœìŠ¤í¬ë¨¸ë¡œ ì˜ˆì¸¡ì„ í•´ë³¼ ë•Œ ì‚¬ìš©í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ config.yaml           # main_build.py ì™€ main_train.pyì— í•„ìš”í•œ ì¸ìë¥¼ ì •ì˜í•´ë†“ëŠ” ì„¤ì •íŒŒì¼
â””â”€â”€ cleanformer          # main ìŠ¤í¬ë¦½íŠ¸ì— ì‚¬ìš©ë  ì¬ë£Œë¥¼ ì •ì˜í•˜ëŠ” íŒŒì´ì¬ íŒ¨í‚¤ì§€
    â”œâ”€â”€ builders.py      # ë§ë­‰ì¹˜ -> ì…ë ¥í…ì„œ, ì •ë‹µí…ì„œ ë³€í™˜ì„ ë„ì™€ì£¼ëŠ” ë¹Œë” ì •ì˜
    â”œâ”€â”€ tensors.py       # íŠ¸ëœìŠ¤í¬ë¨¸ êµ¬í˜„ì— í•„ìš”í•œ ìƒìˆ˜í…ì„œ ì •ì˜ (e.g. subsequent_mask, positional_encoding)
    â”œâ”€â”€ datamodules.py   # í•™ìŠµì— ì‚¬ìš©í•  train/val/test ë°ì´í„° ì •ì˜
    â”œâ”€â”€ models.py        # ëª¨ë“  ì‹ ê²½ë§ ëª¨ë¸ ì •ì˜
    â”œâ”€â”€ fetchers.py      # ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œ ë° ë¡œë“œí•˜ëŠ” í•¨ìˆ˜ ì •ì˜
    â”œâ”€â”€ paths.py         # fetchers.pyê°€ ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œ ë° ë¡œë“œí•  ê²½ë¡œ ì •ì˜
    â””â”€â”€ __init__.py          
```

## íŠ¸ëœìŠ¤í¬ë¨¸ë€?
- [WEEK7: Transformer - why?](https://www.notion.so/WEEK7-Transformer-why-8e3712fb674a4ba2a85bf6da9cd36af0)
- [WEEK8: Transformer: How does Self-attention work?](https://www.notion.so/WEEK8-Transformer-How-does-Self-attention-work-e02fc6b942f64b2ba82ce7e35afc817d)
- [WEEK9: How does Multihead Self attention work?](https://www.notion.so/WEEK9-How-does-Multihead-Self-attention-work-cddce1ae09eb4b0fb067a2474cbf8515)
- [WEEK9: How does Residual Connection & Layer normalisation work?](https://www.notion.so/WEEK9-How-does-Residual-Connection-Layer-normalisation-work-b4a41db45a014378bc1c4a0f6da3757e)
- [WEEK9: How does Positional Encoding  work?](https://www.notion.so/WEEK9-How-does-Positional-Encoding-work-0d0e5b9d17464af08f39b4977c073beb)

